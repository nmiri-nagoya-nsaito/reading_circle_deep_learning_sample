{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 誤差逆伝播法\n",
    "* 前章では重みパラメータの勾配（重みパラメータに関する損失関数の勾配）を数値微分で決定した．\n",
    "* 数値微分は簡単だが計算に時間がかかる\n",
    "* 本章では重みパラメータの効率良い計算法として「誤差逆伝播法」について学ぶ\n",
    "* 誤差逆伝播法を理解する方法\n",
    "  * 数式による方法：一般的な方法．厳密で簡潔．\n",
    "  * 計算グラフ（computational graph）による方法：視覚的，分かりやすい．\n",
    "    * [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)\n",
    "    * [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io)\n",
    "\n",
    "## 5.1 計算グラフ\n",
    "\n",
    "* 計算グラフとは，計算の過程をグラフで表したもの\n",
    "  * グラフとは，複数のノードとエッジ（ノード間を結ぶ直線）によって表現される，データ構造としてのグラフ\n",
    "\n",
    "### 5.1.1 計算グラフで解く\n",
    "\n",
    "問1：太郎くんはスーパーで1個100円のリンゴを2個買いました．支払う金額を求めなさい．ただし，消費税が10％適用されるものとします．\n",
    "\n",
    "* 計算グラフはノードと矢印で計算過程を表す．\n",
    "* ノードは○で表記し，○の中に演算の内容を書く\n",
    "* 計算の途中結果を矢印の上部に書くことで，ノードごとの計算結果が左から右へ伝わるように表す\n",
    "\n",
    "<img src=\"./fig5_1.png\">\n",
    "\n",
    "```\n",
    "# http://viz-js.com/\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n0 [label=\"リンゴ\"]\n",
    "    n1 [label=\"×2\"]\n",
    "    n2 [label=\"×1.1\"]\n",
    "    n3 [label=\"合計\"]\n",
    "\n",
    "    n0 -> n1 [label=\"100\"]\n",
    "    n1 -> n2 [label=\"200\"]\n",
    "    n2 -> n3 [label=\"220\"]\n",
    "}\n",
    "```\n",
    "\n",
    "* 最初にリンゴの100円が「×2」ノードへ流れ，200円になって次のノードの伝達される\n",
    "* その200円が「×1.1」ノードへ流れ，220円になる\n",
    "* この計算グラフの結果から答えは220円になる\n",
    "\n",
    "#### 別の書き方\n",
    "\n",
    "<img src=\"./fig5_2.png\">\n",
    "\n",
    "```\n",
    "# http://viz-js.com/\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n0 [label=\"リンゴ\"]\n",
    "    n01 [label=\"リンゴ\\nの個数\"]\n",
    "    n1 [label=\"×\"]\n",
    "    n11 [label=\"消費税\"]\n",
    "    n2 [label=\"×\"]\n",
    "    n3 [label=\"合計\"]\n",
    "\n",
    "    n0 -> n1 [label=\"100\"]\n",
    "    n01 -> n1 [label=\"2\"]\n",
    "    n1 -> n2 [label=\"200\"]\n",
    "    n11 -> n2 [label=\"1.1\"]\n",
    "    n2 -> n3 [label=\"220\"]\n",
    "}\n",
    "```\n",
    "\n",
    "問2：太郎くんはスーパーでリンゴを2個，みかんを3個買いました．リンゴは1個100円，みかんは1個150円です．消費税が10％かかるものとして，支払う金額を求めなさい．\n",
    "\n",
    "<img src=\"./fig5_3.png\">\n",
    "\n",
    "```\n",
    "# http://viz-js.com/\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n00 [label=\"リンゴ\"]\n",
    "    n01 [label=\"リンゴ\\nの個数\"]\n",
    "    n03 [label=\"みかん\\nの個数\"]\n",
    "    n02 [label=\"みかん\"]\n",
    "\n",
    "    n10 [label=\"×\"]\n",
    "    n11 [label=\"×\"]\n",
    "    \n",
    "    n20 [label=\"+\"]\n",
    "    n21 [label=\"消費税\"]\n",
    "\n",
    "    n3 [label=\"×\"]\n",
    "    n4 [label=\"合計\"]\n",
    "\n",
    "    n00 -> n10 [label=\"100\"]\n",
    "    n01 -> n10 [label=\"2\"]\n",
    "    n02 -> n11 [label=\"150\"]\n",
    "    n03 -> n11 [label=\"3\"]\n",
    "    \n",
    "    n10 -> n20 [label=\"200\"]\n",
    "    n11 -> n20 [label=\"450\"]\n",
    "\n",
    "    n20 -> n3 [label=\"650\"]\n",
    "    n21 -> n3 [label=\"1.1\"]\n",
    "    n3 -> n4 [label=\"715\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### 計算グラフまとめ\n",
    "\n",
    "* 計算グラフを使って問題を解く流れ\n",
    "    1. 計算グラフを構築する\n",
    "    2. 計算グラフ上で計算を左から右へ進める（順伝播 forward propagation という）   \n",
    "\n",
    "### 5.1.2 局所的な計算\n",
    "\n",
    "* 計算グラフの特徴は「局所的な計算」を伝播することにより最終的な結果を得ることができる点\n",
    "  * 局所的とは「自分（ノード）に関係する小さな範囲」ということ．それ以外は気にしない\n",
    "  * 局所的な計算とは「自分（ノード）への入力だけからその結果を出力する」ということ\n",
    "* 全体では複雑な計算であったとしても，分割して単純な局所的計算の集まりにでき，それぞれは個々の計算に集中できる\n",
    "\n",
    "### 5.1.3 なぜ計算グラフで解くのか？\n",
    "\n",
    "* 計算グラフの利点\n",
    "  * 局所的な計算．分割して個々の問題を単純にする．\n",
    "  * 途中の計算結果を保持できる\n",
    "  * **逆方向の伝播によって「微分」を効率良く計算できる**\n",
    "\n",
    "* ここで問1の場合での微分とは何かを考えてみる\n",
    "  * 合計をりんごの値段で微分する場合（つまり，「りんごの値段に関する合計金額の微分」を求める）\n",
    "  * 記号で表現すると「合計金額を$L$,りんごの値段を$x$とするとき$\\frac{\\partial L}{\\partial x}$を求める」ことになる\n",
    "* 微分は逆方向の伝播で計算することができる\n",
    "  * 逆伝播は逆向きの矢印で表現する\n",
    "  * 逆伝播は局所的な微分を表す\n",
    "  * この図の場合，りんごを指す矢印の数値は2.2になる．これは微分が2.2であることを表す．\n",
    "    * つまり，りんごが1円値上がりしたら支払い金額は2.2円増える\n",
    "\n",
    "<img src=\"./fig5_4.png\">\n",
    "\n",
    "```\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n0 [label=\"リンゴ\"]\n",
    "    n01 [label=\"リンゴ\\nの個数\"]\n",
    "    n1 [label=\"×\",color=\"gray\"]\n",
    "    n11 [label=\"消費税\"]\n",
    "    n2 [label=\"×\",color=\"gray\"]\n",
    "    n3 [label=\"合計\"]\n",
    "\n",
    "    n0 -> n1 [label=\"100\",color=\"gray\"]\n",
    "    n01 -> n1 [label=\"2\",color=\"gray\"]\n",
    "    n1 -> n2 [label=\"200\",color=\"gray\"]\n",
    "    n11 -> n2 [label=\"1.1\",color=\"gray\"]\n",
    "    n2 -> n3 [label=\"220\",color=\"gray\"]\n",
    "\n",
    "    n3 -> n2 [label=\"1\",style=\"bold\",color=\"red\"]\n",
    "    n2 -> n1 [label=\"1.1\",style=\"bold\",color=\"red\"]\n",
    "    n1 -> n0 [label=\"2.2\",style=\"bold\",color=\"red\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 連鎖律\n",
    "\n",
    "* 「局所的な微分」を伝達する原理は**連鎖率(chain rule)**による\n",
    "\n",
    "### 5.2.1 計算グラフの逆伝播\n",
    "\n",
    "#### 逆伝播の計算\n",
    "\n",
    "* 信号Eに，関数$f=y(x)$の微分$\\frac{\\partial y}{\\partial x}$を乗算して前のノードへ渡す\n",
    "* これで微分値が効率よく求められる理由は連鎖率の原理にある(次節で説明)\n",
    "\n",
    "<img src=\"fig5_5.png\">\n",
    "\n",
    "### 5.2.2 連鎖律とは\n",
    "\n",
    "#### 合成関数\n",
    "\n",
    "* 合成関数とは，複数の関数によって構成される関数のこと\n",
    "  * たとえば $z={(x+y)}^2$ という式は次の2つの式で構成される\n",
    "    * $z=t^2$\n",
    "    * $t=x+y$\n",
    "\n",
    "#### 連鎖律\n",
    "\n",
    "連鎖律とは，合成関数の微分についての性質のこと\n",
    "\n",
    "#### 連鎖律の原理\n",
    "\n",
    "* ある関数が合成関数で表される場合，その合成関数の微分は，合成関数を構成するそれぞれの関数の微分の席によって表すことができる\n",
    "  * $\\frac{\\partial z}{\\partial x}$ は$\\frac{\\partial z}{\\partial t}$と$\\frac{\\partial t}{\\partial x}$の席で表すことができるということ\n",
    "$$\\frac{\\partial z}{\\partial x} \n",
    "= \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x}$$\n",
    "先の例では$\\frac{\\partial z}{\\partial t}=2t$,$\\frac{\\partial t}{\\partial x}=1$が解析的に得られたため\n",
    "$$\\frac{\\partial z}{\\partial x} = 2(x+y)$$\n",
    "\n",
    "### 5.2.3 連鎖律と計算グラフ\n",
    "\n",
    "$t=(x+y)^2$をノードで表してみる\n",
    "\n",
    "<img src=\"fig5_6.png\">\n",
    "\n",
    "* 逆伝播は右から左へと信号が伝播する\n",
    "* 各ノードはそれまでの偏微分の積を受け取り，それに自ノードの偏微分を積算して次（前段）のノードへ送る\n",
    "* 自ノードの偏微分を積算することで，最終出力を自ノードの入力で偏微分した結果が得られることになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 逆伝播\n",
    "\n",
    "### 5.3.1 加算ノードの逆伝播\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}=1$, $\\frac{\\partial z}{\\partial y}=1$であるため，上流から伝わった微分を$\\frac{\\partial L}{\\partial z}$とすると次のようになる．\n",
    "\n",
    "<img src=\"fig5_7.png\">\n",
    "\n",
    "つまり，加算ノードの逆伝播は入力の内容をそのまま出力するだけ．\n",
    "\n",
    "```\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n0 [label=\"\", color=\"none\"]\n",
    "    n01 [label=\"\", color=\"none\"]\n",
    "    n1 [label=\"+\",color=\"gray\"]\n",
    "    n2 [label=\"\",color=\"none\"]\n",
    "\n",
    "    { rank = same;\n",
    "        \"n0\", \"n01\";\n",
    "    }\n",
    "\n",
    "    n0 -> n1 [label=\"x\",color=\"gray\"]\n",
    "    n01 -> n1 [label=\"y\",color=\"gray\"]\n",
    "    n1 -> n2 [label=\"z\",color=\"gray\"]\n",
    "\n",
    "    n1 -> n0 [label=\"dL/dz・1 \",style=\"bold\",color=\"red\"]\n",
    "    n1 -> n01 [label=\"dL/dz・1 \",style=\"bold\",color=\"red\"]\n",
    "    n2 -> n1 [label=\"dL/dz\",style=\"bold\",color=\"red\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### 5.3.2 乗算ノードの逆伝播\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}=y$, $\\frac{\\partial z}{\\partial y}=x$であるため，上流から伝わった微分を$\\frac{\\partial L}{\\partial z}$とすると次のようになる．\n",
    "\n",
    "<img src=\"fig5_8.png\">\n",
    "\n",
    "乗算ノードの逆伝播は入力の内容に入力をひっくり返した値を乗算して出力する．\n",
    "（多入力の乗算の場合は？2入力の乗算に分解すればいいということか）\n",
    "\n",
    "### 5.3.3 リンゴの例\n",
    "\n",
    "先のりんごとみかんの例について逆伝播をためしてみる．\n",
    "\n",
    "<img src=\"fig5_9.png\">\n",
    "\n",
    "```\n",
    "# http://viz-js.com/\n",
    "digraph G {\n",
    "    rankdir=LR;\n",
    "    node [shape=circle]\n",
    "    n00 [label=\"リンゴ\"]\n",
    "    n01 [label=\"リンゴ\\nの個数\"]\n",
    "    n03 [label=\"みかん\\nの個数\"]\n",
    "    n02 [label=\"みかん\"]\n",
    "\n",
    "    n10 [label=\"×\"]\n",
    "    n11 [label=\"×\"]\n",
    "\n",
    "    n20 [label=\"+\"]\n",
    "    n21 [label=\"消費税\"]\n",
    "\n",
    "    n3 [label=\"×\"]\n",
    "    n4 [label=\"合計\"]\n",
    "\n",
    "    { rank = same; \"n00\", \"n01\", \"n02\", \"n03\"; }\n",
    "    { rank = same; \"n10\", \"n11\"; }\n",
    "    { rank = same; \"n20\", \"n21\"; }\n",
    "    { rank = same; \"n3\"; }\n",
    "    { rank = sink; \"n4\"; }\n",
    "\n",
    "    n00 -> n10 [label=\"100\"]\n",
    "    n01 -> n10 [label=\"2\"]\n",
    "    n02 -> n11 [label=\"150\"]\n",
    "    n03 -> n11 [label=\"3\"]\n",
    "    n10 -> n20 [label=\"200\"]\n",
    "    n11 -> n20 [label=\"450\"]\n",
    "    n20 -> n3 [label=\"650\"]\n",
    "    n21 -> n3 [label=\"1.1\"]\n",
    "    n3 -> n4 [label=\"715\"]\n",
    "\n",
    "    n4 -> n3 [label=\"1\", color=\"red\", style=\"bold\"]\n",
    "    n3 -> n20 [label=\"1.1\", color=\"red\", style=\"bold\"]\n",
    "    n3 -> n21 [label=\"650\", color=\"red\", style=\"bold\"]\n",
    "    n20 -> n10 [label=\"1.1\", color=\"red\", style=\"bold\"]\n",
    "    n20 -> n11 [label=\"1.1\", color=\"red\", style=\"bold\"]\n",
    "    n10 -> n00 [label=\"2\", color=\"red\", style=\"bold\"]\n",
    "    n10 -> n01 [label=\"110\", color=\"red\", style=\"bold\"]\n",
    "    n11 -> n02 [label=\"3.3\", color=\"red\", style=\"bold\"]\n",
    "    n11 -> n03 [label=\"165\", color=\"red\", style=\"bold\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 単純なレイヤの実装\n",
    "\n",
    "### 5.4.1 乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 乗算レイヤの実装\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# 乗算レイヤを使ったりんごの例を計算してみる\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer の定義\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# りんごの例で，逆伝播から微分を求めてみる\n",
    "# dapple_price, dtax はそれぞれ apple_price, tax の微分を表す\n",
    "dprice = 1\n",
    "\n",
    "# 呼び出す順番は forward の時と逆の順番で行う\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加算レイヤの実装\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n"
     ]
    }
   ],
   "source": [
    "# りんごとみかんの買い物の例を実装する\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer の定義\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 3.3000000000000003 165.0 1.1 1.1 1.1 650\n"
     ]
    }
   ],
   "source": [
    "# 微分を計算してみる\n",
    "\n",
    "dprice = 1\n",
    "\n",
    "# backword\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(dapple, dapple_num, dorange, dorange_num, dapple_price, dorange_price, dall_price, dtax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 活性化関数レイヤの実装\n",
    "\n",
    "### 5.5.1 ReLUレイヤ\n",
    "\n",
    "ReLU(Rectified Linear Unit) の式\n",
    "\n",
    "$$y = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "        x & (x>0) \\\\\n",
    "        0 & (x \\leq 0) \\\\\n",
    "    \\end{array} \\right.\n",
    "$$\n",
    "ここで$y$を$x$で偏微分すると\n",
    "$$\\frac{\\partial y}{\\partial x} = \\left\\{ \n",
    "    \\begin{array}{ll}\n",
    "        1 & (x>0) \\\\\n",
    "        0 & (x \\leq 0) \\\\\n",
    "    \\end{array} \\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ReLUレイヤの実装\n",
    "# ここで forward および backward にはNumPyの配列が入力されることを想定する\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        # マスクされる要素のインデックスのみTrueでそれ以外はFalseのNumpy配列\n",
    "        self.mask = None\n",
    "    \n",
    "    # 入力xの要素のうち，0以下の要素のみ0にして出力する\n",
    "    def forward(self, x):\n",
    "        # 0以下の値を持つ要素のみTrueを持つ配列を作る\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # doutの中で，マスクする要素のみ0にする．それ以外はそのままの値とする\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoidレイヤ\n",
    "\n",
    "シグモイド関数の定義\n",
    "$$y=\\frac{1}{1+\\exp (-x)}$$\n",
    "\n",
    "およびその偏微分は次のようになる．\n",
    "$$\\frac{\\partial y}{\\partial x}=y^2\\exp(-x)$$\n",
    "\n",
    "この関数の計算グラフおよび微分を計算すると，次のようになる\n",
    "<img src=\"fig5_10.png\">\n",
    "出力$y$の$x$による偏微分の式は$x$と$y$のみからなるため，次のように計算グラフを簡略化できる\n",
    "<img src=\"fig5_11.png\">\n",
    "\n",
    "さらに式を変形する\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\frac{\\partial L}{\\partial y}y^2\\exp(-x) &=& \\frac{\\partial L}{\\partial y}\\frac{1}{(1+\\exp(-x))^2}\\exp(-x) \\\\\n",
    "&=& \\frac{\\partial L}{\\partial y}\\frac{1}{1+\\exp(-x)}\\frac{\\exp(-x)}{1+\\exp(-x)} \\\\\n",
    "&=& \\frac{\\partial L}{\\partial y}{y(1-y)} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "つまり，Sigmoid関数の微分は順方向の出力値だけを使って計算することができる\n",
    "<img src=\"fig5_12.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoidレイヤの実装\n",
    "# ここで forward および backward にはNumPyの配列が入力されることを想定する\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        #　逆伝播のときに使えるよう記憶しておく\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmaxレイヤの実装\n",
    "\n",
    "### 5.6.1 Affineレイヤ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 バッチ版Affineレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1 2]\n",
      "   [3 4]\n",
      "   [5 6]]\n",
      "\n",
      "  [[4 5]\n",
      "   [6 7]\n",
      "   [8 9]]]\n",
      "\n",
      "\n",
      " [[[1 2]\n",
      "   [3 4]\n",
      "   [5 6]]\n",
      "\n",
      "  [[4 5]\n",
      "   [6 7]\n",
      "   [8 9]]]]\n",
      "(2, 2, 3, 2)\n",
      "2\n",
      "[[1 2 3 4 5 6 4 5 6 7 8 9]\n",
      " [1 2 3 4 5 6 4 5 6 7 8 9]]\n",
      "(2, 12)\n"
     ]
    }
   ],
   "source": [
    "# テンソル対応でやってること\n",
    "# 本では3次元以上の配列のことをテンソルと呼んでいた\n",
    "# 例として4次元配列を与えてみる\n",
    "X = np.array([[[[1,2],[3,4],[5,6]],[[4,5],[6,7],[8,9]]],[[[1,2],[3,4],[5,6]],[[4,5],[6,7],[8,9]]]])\n",
    "print(X)\n",
    "print(X.shape)\n",
    "# 一番外側の要素の個数\n",
    "print(X.shape[0])\n",
    "# 強制的に2次元配列にする．第2引数で-1を指定すると，第1引数を元に自然な形状の配列を作る\n",
    "print(X.reshape(X.shape[0],-1))\n",
    "print(X.reshape(X.shape[0],-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 誤差逆伝播法の実装\n",
    "\n",
    "### 5.7.1 ニューラルネットワークの学習の全体図\n",
    "\n",
    "### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "#from deep_learning_from_scratch.common.layers import *\n",
    "from deep_learning_from_scratch.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "    \n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.38283401145e-13\n",
      "b1:9.48171263726e-13\n",
      "W2:8.13011583756e-13\n",
      "b2:1.19459989123e-10\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist\n",
    "#from deep_learning_from_scratch.ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1407 0.1424\n",
      "0.90615 0.9085\n",
      "0.92475 0.9258\n",
      "0.933266666667 0.9343\n",
      "0.944533333333 0.9435\n",
      "0.950683333333 0.9494\n",
      "0.956583333333 0.9535\n",
      "0.961516666667 0.9578\n",
      "0.9648 0.9601\n",
      "0.967933333333 0.9636\n",
      "0.969683333333 0.9654\n",
      "0.972783333333 0.967\n",
      "0.973083333333 0.9665\n",
      "0.9744 0.9686\n",
      "0.976216666667 0.9693\n",
      "0.977383333333 0.9708\n",
      "0.9789 0.9705\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 学習履歴\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiLiSwEhIiK+FBAiIuLr/wGcWgbGpMNNwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2030b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 損失関数の推移をグラフに描画して，徐々に下がっていくことを確認\n",
    "x = np.arange(iters_num)\n",
    "plt.plot(x, train_loss_list)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlim(0, 10000)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117079400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 認識精度の推移をグラフに描画し，認識精度が上がっていることを確認\n",
    "# 計算自体は先のプログラムで行なっているため，ここではプロットのみ\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 まとめ\n",
    "\n",
    "* 視覚的に計算の価値を表す計算グラフを学んだ\n",
    "* 計算グラフを用いてニューラルネットワークで行う逆誤差伝搬法を説明し，ニューラルネットワークで行う処理をレイヤという単位で実装した\n",
    "    * ReLUレイヤ，Softmax-with-Lossレイヤ，Affineレイヤ，Softmaxレイヤ\n",
    "    * レイヤにはforwardとbackwardというメソッドが実装されている\n",
    "    * データを順方向と逆方向に伝搬することで重みパラメータの勾配を効率的に求めることができる\n",
    "* レイヤによるモジュール化によりレイヤを自由に組み合わせることができ，好きなネットワークを簡単に作ることができる\n",
    "\n",
    "* 計算グラフを用いると計算過程を簡単に把握できる\n",
    "* 計算グラフのノードは局所的な計算により構成される．局所的な計算が全体の計算を構成する．\n",
    "* 計算グラフの順伝搬は通常の計算を行う．\n",
    "* 計算グラフの逆伝搬によって微分を求めることができる\n",
    "* ニューラルネットワークの構成要素をレイヤとして実装することで，勾配計算を効率的に求めることができる（逆誤差伝搬法）\n",
    "* 数値微分と逆誤差伝搬法の結果を比較することで，逆誤差伝搬法の実装に誤りがないことを確認することができる（勾配確認）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
