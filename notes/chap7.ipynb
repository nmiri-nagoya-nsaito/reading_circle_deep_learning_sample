{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7章 畳み込みニューラルネットワーク(convolutional neural network: CNN)\n",
    "* CNNは画像認識や音声認識などいたるところで使われている\n",
    "* 画像認識のコンペでは，ほとんどすべてがCNNベースである\n",
    "\n",
    "## 7.1 全体の構造\n",
    "* CNNもこれまでのニューラルネットワークと同じくレイヤを組み合わせて作る\n",
    "* CNNでは「Convolutionレイヤ（畳み込み層）」と「Poolingレイヤ（プーリング層）」が加わる\n",
    "* これまでのネットワークでは隣接する層の全ニューロン間が結合されていた（全結合：fully-connected）\n",
    "  * 全結合層はAffineレイヤという名称で実装した\n",
    "  * Affineレイヤの後にReLUを接続し，それを何回か繰り返したのち，最終出力では Affineレイヤ$\\rightarrow$Softmaxレイヤの順で出力する\n",
    "* CNNの場合の構成\n",
    "  * Convolutionレイヤ$\\rightarrow$ReLUレイヤ$\\rightarrow$Poolingレイヤの順で接続し，それを何回か繰り返し，最終出力では全結合ネットワークと同様にAffineレイヤ$\\rightarrow$Softmaxレイヤで出力する\n",
    "  * Poolingレイヤは省略されることもある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 畳み込み層\n",
    "### 7.2.1 全結合層の問題点\n",
    "* 全結合層のニューラルネットワークではAffineレイヤを使用した\n",
    "  * 隣接する層のニューロンが全て連結される\n",
    "  * 出力の数は任意に決めることができる\n",
    "* Affineレイヤの問題点は，データの形状が無視されること\n",
    "  * 例）画像は本来「縦，横，チャネル」の方向の広がりを持った3次元形状のデータである\n",
    "  * 3次元データをAffineレイヤへ入力するには，データを1次元に変換する必要がある\n",
    "  * 画像では隣接するピクセル間に類似性があるなど，なんらかのパターンが含まれる\n",
    "  * Affineレイヤはパターンを無視して入力データを扱い，パターンの情報を生かすことがない\n",
    "* 畳み込み層は形状を維持\n",
    "  * 画像の場合，入力を3次元データとして受け取り，3次元データとして次の層に出力\n",
    "    * （参考）CNNでは畳み込み層の入出力データのことを**特徴マップ（feature map）**，**入力特徴マップ**，**出力特徴マップ**のように呼ぶ場合がある\n",
    "  * 形状を維持するため，形状やパターンのデータをうまく扱える可能性が残る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 畳み込み演算\n",
    "* 畳み込み層では畳み込み演算を行う\n",
    "  * 畳み込み演算は画像処理でいうところのフィルター演算に相当する\n",
    "* 畳み込み演算は，「入力データ」に対して「フィルター」を適用する\n",
    "  * 入力データおよびフィルタはともに縦横の形状をもつ\n",
    "  * フィルタは「カーネル」と呼ばれることもある\n",
    "* 畳み込み演算は入力データに対してフィルターのウィンドウを一定間隔でスライドさせながら適用させる\n",
    "  * それぞれの場所でフィルタの要素と入力の対応する要素を乗算し，その和を計算する（積和演算）\n",
    "  * その和の値を出力の対応する場所へ格納する\n",
    "  * このプロセスをすべての場所で行うことで畳み込み演算の出力を得る\n",
    "* Affineレイヤでは重みパラメータの他にバイアスが存在した．\n",
    "* CNNの場合，フィルタのパラメータが重みに対応する．またバイアスも存在する\n",
    "  * バイアスは(1,1)のデータで，フィルタ適用後の結果の全要素にそれぞれ加算される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 パディング(padding)\n",
    "* 出力サイズを調整するために，入力データの周囲に固定のデータを埋めること\n",
    "  * 例えば「幅1のパディングを適用する」とは，周囲を幅1ピクセルの0で埋める\n",
    "* パディングすると出力は大きくなる\n",
    "\n",
    "### 7.2.4 ストライド(stride)\n",
    "* フィルターを適用する位置の間隔\n",
    "* 例えば，ストライドを2にすると，積和演算のためのフィルタは2要素ずつ移動する\n",
    "* ストライドを大きくすると出力は小さくなる\n",
    "* サイズの関係: $OH, OW$の計算結果は整数にならなければならないことに注意（割り切れること）\n",
    "  * 入力サイズ $(H,W)$\n",
    "  * フィルターサイズ $(FH,FW)$\n",
    "  * 出力サイズ $(OH, OW)$\n",
    "  * パディング $P$\n",
    "  * ストライド $S$\n",
    "$$OH=\\frac{H+2P-FH}{S}+1 \\\\\n",
    "OW=\\frac{W+2P-FW}{S}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 3次元データの畳み込み演算\n",
    "* 画像の場合は縦横だけでなくチャネル方向も合わせた3次元で扱う必要がある\n",
    "* チャネル方向も合わせた3次元データに対する畳み込み演算を考える\n",
    "* チャネルごとに特徴マップがある場合，チャネルごとに入力データとフィルタの畳み込み演算を行い，チャネルごとの結果を加算して一つの出力を計算する\n",
    "* 注意点：入力データとフィルタのチャネル数は同じにする\n",
    "  * フィルタのサイズは自由に設定できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.6 ブロックで考える\n",
    "* 前節では複数チャネル（チャネル数 $C$）からなる画像データ(サイズ $H \\times W$)に，それと同じチャネル数($C$)のフィルタ(サイズ $FH \\times FW$)を適用して一つの（チャネルが1の）出力特徴マップ(サイズ $OH \\times OW$)を得た\n",
    "* 複数チャネル($FN$個)からなる出力特徴マップを得るにはどうしたら良いか $\\rightarrow$ 複数($FN$個)のフィルタを用いる\n",
    "* 畳み込み演算のフィルタはフィルタの個数$(FN)$も考慮すると4次元のデータとなる\n",
    "  * (出力チャネル数，入力チャネル数，出力縦，出力横)\n",
    "    * チャネル数3, サイズ5$\\times$5 のフィルタが20個ある場合は $(20, 3, 5, 5)$\n",
    "* 畳み込み演算のバイアスは出力チャネル数を考慮すると (FN, 1, 1) となる\n",
    "  * チャネルごとに同じバイアス値がチャネル内の全要素に加算される\n",
    "* まとめ\n",
    "  * 入力特徴マップ $(C, H, W)$\n",
    "  * フィルタ $(FN, C, FH, FW)$\n",
    "  * バイアス $(FN, 1, 1)$\n",
    "  * 出力特徴マップ $(FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.7 バッチ処理\n",
    "* バッチを考慮しない場合の入力特徴マップは $(C, H, W)$，出力は $(FN, OH, OW)$\n",
    "* バッチ処理を考慮する場合，複数($N$個)のデータをまとめて処理することになり，それが畳み込み層へ入力される\n",
    "* したがってバッチ処理を考慮すると，レイヤの特徴マップは4次元になる：入力 $(N, C, H, W)$ および出力 $(N, FN, OH, OW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 プーリング層\n",
    "* プーリングは縦・横方向の空間を小さくする演算\n",
    "* 複数の要素を一つに集約するような処理を行うことで，空間を小さくする\n",
    "* 最大値をとるMaxプーリングや平均をとるAverageプーリングなどがある\n",
    "  * ここではMaxプーリングのみを使用する\n",
    "\n",
    "### 7.3.1 プーリング層の特徴\n",
    "* 学習するパラメータがない\n",
    "  * 入力データだけを使って行う処理なので，学習が必要ない\n",
    "* チャネル数は変化しない\n",
    "  * チャネルごとに処理を行い，まとめないためチャネル数は変わらない\n",
    "* 微小な位置変化に対してロバストである\n",
    "  * 多少ずれても同じような結果となる（必ずしも同じにはならないことに注意）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.4 Convolution/Poolingレイヤの実装\n",
    "* レイヤなのでこれまでと同様，forward と backward というメソッドを持つ\n",
    "\n",
    "### 7.4.1 4次元配列\n",
    "* CNNの各層を流れるデータは（バッチおよびチャネルを考慮して）4次元データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# 以降の実装のための準備\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.append(os.pardir + \"/deep_learning_from_scratch\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 5, 5)\n",
      "[[[ 0.14084385  0.44877728  0.08259538  0.26756363  0.51643597]\n",
      "  [ 0.23653813  0.83853972  0.18114949  0.04358095  0.62453826]\n",
      "  [ 0.27176381  0.21895837  0.32422096  0.18849354  0.51831539]\n",
      "  [ 0.91518921  0.45949892  0.39380019  0.64429486  0.61655682]\n",
      "  [ 0.39665545  0.62127406  0.36000038  0.6740223   0.47409886]]]\n",
      "[[ 0.14084385  0.44877728  0.08259538  0.26756363  0.51643597]\n",
      " [ 0.23653813  0.83853972  0.18114949  0.04358095  0.62453826]\n",
      " [ 0.27176381  0.21895837  0.32422096  0.18849354  0.51831539]\n",
      " [ 0.91518921  0.45949892  0.39380019  0.64429486  0.61655682]\n",
      " [ 0.39665545  0.62127406  0.36000038  0.6740223   0.47409886]]\n"
     ]
    }
   ],
   "source": [
    "# 適当な4次元配列をためしてみる\n",
    "\n",
    "# バッチサイズ10，チャネル1，縦横それぞれ5のデータとして生成\n",
    "x = np.random.rand(10, 1, 5, 5)\n",
    "\n",
    "# 形状を確認\n",
    "print(x.shape)\n",
    "\n",
    "# 一つ目のデータにアクセス．この場合 (1, 5, 5) のサイズの配列が表示される\n",
    "print(x[0])\n",
    "# 一つ目のデータの最初のチャネルデータにアクセス．(5, 5)のサイズの配列が表示される\n",
    "print(x[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 im2col による展開\n",
    "* 畳み込み演算を愚直に実装するとforを多段に入れ子にした（ネストした）コードになる．\n",
    "  * Numpyでは性能が問題となる\n",
    "* そこで，for の代わりに im2col という関数を使用する\n",
    "  * 元の行列から，畳み込み領域の要素を1行に展開した行列を生成する\n",
    "  * ストライドが小さい場合，畳み込みの各領域が重なりを持つため，結果の行列は重複要素を含み，サイズが大きくなる\n",
    "  * サイズが大きいとメモリ使用量が増大する点が難点だが，既存の線形代数ライブラリを有効活用できるという利点がある\n",
    "* 既存の[Caffe](http://caffe.berkeleyvision.org/)や[Chainer](https://chainer.org)などのフレームワークでも同様の関数が用意されていて，畳み込み層の実装で用いられている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### im2col を利用した畳み込み計算の概要\n",
    "例として次のような畳み込みを計算する場合を考える．\n",
    "<img src=\"./fig7_1.png\">\n",
    "\n",
    "畳み込み演算を単なる行列の積として計算するために入力データを行列に変換する．\n",
    "\n",
    "1回のフィルター適用領域を1行に展開する．\n",
    "<img src=\"./fig7_2.png\">\n",
    "\n",
    "結果として得られる行列の形状は次のように変化する．\n",
    "<img src=\"./fig7_3.png\">\n",
    "\n",
    "同様に，フィルタも行列の積演算として処理するために変換する．\n",
    "<img src=\"./fig7_4.png\">\n",
    "\n",
    "行列計算によって得られたベクトル（1階のテンソル）を出力の形状に戻す．\n",
    "<img src=\"./fig7_5.png\">\n",
    "\n",
    "チャネル数が増える場合を考える．チャネル数が増えた場合，各チャネル毎の計算結果を足し合わせるため，同じ行に別チャネルの列を追加するように配置する．フィルタもそれに合わせて配置する．\n",
    "<img src=\"./fig7_6.png\">\n",
    "\n",
    "さらに，データ数及びフィルタ数が増えた場合．データ数が増えた場合は行方向に行列が大きくなる．フィルタ数を増やした場合は，列方向にフィルタの行列が大きくなる．\n",
    "<img src=\"./fig7_7.png\">\n",
    "\n",
    "演算結果は（N\\*OH\\*OW,FN）の2次元配列として得られる．これを(N,FN,OH,OW)の形式に変換する\n",
    "<img src=\"./fig7_8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    \n",
    "    # 入力データのサイズを取得\n",
    "    N, C, H, W = input_data.shape\n",
    "    \n",
    "    # 出力画像サイズの計算\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    \n",
    "    # 入力画像の縦横(インデックス2と3)の両端にpad個ずつ定数(0)をパディング\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    # 展開後の入力データを格納する行列 col の生成\n",
    "    # (参考) https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    # 入力画像の要素をコピー\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    \n",
    "    # 軸の順番の入れ替えおよび2次元配列化．\n",
    "    # 2次元配列の最初の軸は出力データに関するもの，後の軸は積和計算に関するもの．\n",
    "    #   行数：（データ数＊出力縦＊出力横）\n",
    "    #   列数：（フィルタ縦＊フィルタ横＊チャネル数）\n",
    "    # 2次元配列化により畳み込みの積和演算が，内積演算で処理できる．\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5, 5)\n",
      "img1.shape = (4, 1, 5, 5)\n",
      "[[[[ 0.28981795  0.94335224  0.16759254  0.51064916  0.51907792]\n",
      "   [ 0.88910699  0.67006615  0.93838163  0.91331215  0.1362478 ]\n",
      "   [ 0.86330053  0.73345959  0.43265135  0.64279587  0.67624462]\n",
      "   [ 0.46676798  0.79193652  0.31141779  0.09006213  0.21030867]\n",
      "   [ 0.19947278  0.20238957  0.66646271  0.30048313  0.76573688]]]\n",
      "\n",
      "\n",
      " [[[ 0.71151034  0.73488454  0.12920365  0.08388241  0.61698579]\n",
      "   [ 0.65991212  0.25294023  0.52659918  0.52819977  0.65296523]\n",
      "   [ 0.52702705  0.99782246  0.62043388  0.23669227  0.18469759]\n",
      "   [ 0.60886588  0.9689133   0.87950377  0.29845623  0.65828543]\n",
      "   [ 0.69721743  0.14873872  0.46249634  0.5700789   0.35870759]]]\n",
      "\n",
      "\n",
      " [[[ 0.65883921  0.19150488  0.82305559  0.99961838  0.27046773]\n",
      "   [ 0.93929027  0.5125369   0.07273421  0.22998071  0.67924134]\n",
      "   [ 0.42951898  0.96056655  0.88902344  0.6062448   0.92113958]\n",
      "   [ 0.85502057  0.09458369  0.44427512  0.20961625  0.61240537]\n",
      "   [ 0.70254245  0.2229548   0.98952363  0.66881215  0.69677965]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]]]]\n",
      "img2.shape = (3, 2, 5, 7)\n",
      "[[[[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.28981795  0.94335224  0.16759254  0.51064916  0.51907792\n",
      "     0.        ]\n",
      "   [ 0.          0.88910699  0.67006615  0.93838163  0.91331215  0.1362478\n",
      "     0.        ]\n",
      "   [ 0.          0.86330053  0.73345959  0.43265135  0.64279587  0.67624462\n",
      "     0.        ]\n",
      "   [ 0.          0.46676798  0.79193652  0.31141779  0.09006213  0.21030867\n",
      "     0.        ]\n",
      "   [ 0.          0.19947278  0.20238957  0.66646271  0.30048313  0.76573688\n",
      "     0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.71151034  0.73488454  0.12920365  0.08388241  0.61698579\n",
      "     0.        ]\n",
      "   [ 0.          0.65991212  0.25294023  0.52659918  0.52819977  0.65296523\n",
      "     0.        ]\n",
      "   [ 0.          0.52702705  0.99782246  0.62043388  0.23669227  0.18469759\n",
      "     0.        ]\n",
      "   [ 0.          0.60886588  0.9689133   0.87950377  0.29845623  0.65828543\n",
      "     0.        ]\n",
      "   [ 0.          0.69721743  0.14873872  0.46249634  0.5700789   0.35870759\n",
      "     0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.65883921  0.19150488  0.82305559  0.99961838  0.27046773\n",
      "     0.        ]\n",
      "   [ 0.          0.93929027  0.5125369   0.07273421  0.22998071  0.67924134\n",
      "     0.        ]\n",
      "   [ 0.          0.42951898  0.96056655  0.88902344  0.6062448   0.92113958\n",
      "     0.        ]\n",
      "   [ 0.          0.85502057  0.09458369  0.44427512  0.20961625  0.61240537\n",
      "     0.        ]\n",
      "   [ 0.          0.70254245  0.2229548   0.98952363  0.66881215  0.69677965\n",
      "     0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "# numpy.pad 関数の動作確認\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
    "x = np.random.rand(3, 1, 5, 5)\n",
    "print(x.shape)\n",
    "img1 = np.pad(x, [(0,1), (0,0), (0, 0), (0, 0)], 'constant')\n",
    "print(\"img1.shape = \" + str(img1.shape))\n",
    "print(img1)\n",
    "\n",
    "img2 = np.pad(x, [(0,0), (1,0), (0, 0), (1, 1)], 'constant')\n",
    "print(\"img2.shape = \" + str(img2.shape))\n",
    "print(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 3)\n",
      "[[[ 1.  1.  5.]]\n",
      "\n",
      " [[ 1.  3.  1.]]]\n",
      "x2.shape = (3, 2, 1)\n",
      "x2 = [[[ 1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 3.]]\n",
      "\n",
      " [[ 5.]\n",
      "  [ 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# numpy.transpose の動作確認\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.transpose.html\n",
    "# 要素が全て1の3次元配列を生成し，形状を確認\n",
    "x = np.ones((2,1,3))\n",
    "print(x.shape)\n",
    "\n",
    "# 特定の要素のみ値を変えてみる\n",
    "x[0][0][2] = 5\n",
    "x[1][0][1] = 3\n",
    "print(x)\n",
    "\n",
    "# 形状を変換する．3番目の軸を1番目に，1番目の軸を2番目に，2番目の軸を3番目に\n",
    "x2 = x.transpose(2,0,1)\n",
    "print(\"x2.shape = \" + str(x2.shape))\n",
    "print(\"x2 = \" + str(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Convolutionレイヤの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 7, 7)\n",
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# im2colを使ってみる\n",
    "# 入力画像サイズ(7,7)，チャネル数3，データ数1\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "# フィルタのサイズ(5,5), パディングなし，ストライド1\n",
    "print(x1.shape)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "# 形状を出力．(データ数*出力縦*出力横，チャネル数*フィルタ縦*フィルタ横) となるはず．\n",
    "# 出力画像サイズが(3,3)になるため，その場合，col1のサイズは(1*3*3, 3*5*5)=(9,75)\n",
    "print(col1.shape)\n",
    "\n",
    "# データ10個とする場合．単純に結果の行数が10倍になる\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# com2im の実装(common/utils.pyより)\n",
    "# 畳み込みレイヤおよびPoolingレイヤの逆伝播で使用する\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "    \n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n",
      "(10, 3, 7, 7)\n"
     ]
    }
   ],
   "source": [
    "# col2im を使ってみる\n",
    "\n",
    "# 行列サイズ(10*3*3, 3*5*5)\n",
    "# N=10, C=3, OH=3, OW=3, FH=5, FW=5\n",
    "col1 = np.random.rand(90, 75)\n",
    "print(col1.shape)\n",
    "\n",
    "# 入力サイズ(N=10, C=3, H=7, W=7), FH=5, FW=5, ストライド1, パディングなし\n",
    "im1 = col2im(col1, (10,3,7,7), 5, 5, stride=1, pad=0)\n",
    "\n",
    "# 形状を出力．(データ数, チャネル数, 画像縦, 画像横) となる\n",
    "print(im1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 畳み込み層の実装\n",
    "class Convolution:\n",
    "    # W: フィルタ．(データ数，チャネル数，フィルタ高さ，フィルタ幅)の4次元配列\n",
    "    # b: バイアス．データ数の1次元配列\n",
    "    # stride: ストライド（既定値1）\n",
    "    # pad: パディング幅(既定値0)\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    # 順方向処理\n",
    "    # x: 入力データ．（データ数，チャネル数，高さ，幅）の4次元配列\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        # 入力画像からフィルタリング用の2次元配列を生成\n",
    "        # 形状は（データ数＊出力高さ＊出力幅，チャネル数＊フィルタ高さ＊フィルタ幅）\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # フィルタの展開．（1個あたりFH*FW*Cのサイズの列ベクトルがFN個並んだ行列になる）\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        # 4次元配列に戻し，軸の順序を入れ替え．4番目の軸(フィルタ個数)を2番目にもってくる．\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # 逆伝播\n",
    "    # Affineレイヤとほぼ同様\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Poolingレイヤの実装\n",
    "* Convolutionレイヤと同様にim2colを使って入力データを展開する\n",
    "* プーリングの場合はチャンネル方向には独立であるため，プーリングの適用領域はチャンネルごとに独立して展開する\n",
    "* 展開後は，展開した行列に対して，行ごとに最大値を計算し，適切な形状に整形するだけ\n",
    "* Poolingレイヤ実装の流れ\n",
    "  1. 入力データを展開する\n",
    "  2. 行ごとに最大を求める\n",
    "    * 最大値の計算には numpy.max 関数が利用できる．\n",
    "  3. 適切な出力サイズに整形する\n",
    "* 逆伝播は ReLUレイヤの説明が参考になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "    \n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        \n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29982078687\n",
      "=== epoch:1, train acc:0.097, test acc:0.093 ===\n",
      "train loss:2.29814264546\n",
      "train loss:2.29312690768\n",
      "train loss:2.29130448073\n",
      "train loss:2.28486122169\n",
      "train loss:2.27396438766\n",
      "train loss:2.2609443809\n",
      "train loss:2.2547972833\n",
      "train loss:2.22749704666\n",
      "train loss:2.19238656437\n",
      "train loss:2.18857932475\n",
      "train loss:2.14215049867\n",
      "train loss:2.12754425337\n",
      "train loss:2.11193860628\n",
      "train loss:2.04760611497\n",
      "train loss:1.94512738092\n",
      "train loss:1.94825039358\n",
      "train loss:1.83961362176\n",
      "train loss:1.80463576908\n",
      "train loss:1.76130594112\n",
      "train loss:1.61866591206\n",
      "train loss:1.57167187541\n",
      "train loss:1.45162544261\n",
      "train loss:1.47890638082\n",
      "train loss:1.27823143246\n",
      "train loss:1.1819408184\n",
      "train loss:1.20947187787\n",
      "train loss:1.14849309266\n",
      "train loss:1.00615921432\n",
      "train loss:0.925956617895\n",
      "train loss:1.04173659917\n",
      "train loss:0.950833135383\n",
      "train loss:0.806805901061\n",
      "train loss:0.707438734183\n",
      "train loss:0.968911716525\n",
      "train loss:0.836264245118\n",
      "train loss:0.655616198161\n",
      "train loss:0.660431518788\n",
      "train loss:0.788586195611\n",
      "train loss:0.797163804606\n",
      "train loss:0.753430880465\n",
      "train loss:0.592980012364\n",
      "train loss:0.628563959467\n",
      "train loss:0.601314758587\n",
      "train loss:0.63804199268\n",
      "train loss:0.501662954242\n",
      "train loss:0.649128627684\n",
      "train loss:0.434253288484\n",
      "train loss:0.795277517528\n",
      "train loss:0.579538297153\n",
      "train loss:0.490793362228\n",
      "train loss:0.651710974283\n",
      "train loss:0.591559970804\n",
      "train loss:0.494902713754\n",
      "train loss:0.624261279911\n",
      "train loss:0.422344712559\n",
      "train loss:0.556737785696\n",
      "train loss:0.597145370202\n",
      "train loss:0.421030526226\n",
      "train loss:0.673769401205\n",
      "train loss:0.627619658223\n",
      "train loss:0.493441268537\n",
      "train loss:0.554481813178\n",
      "train loss:0.410239010654\n",
      "train loss:0.562914535001\n",
      "train loss:0.34282782662\n",
      "train loss:0.456625603151\n",
      "train loss:0.62607105975\n",
      "train loss:0.424693724596\n",
      "train loss:0.275339584766\n",
      "train loss:0.434678862356\n",
      "train loss:0.45278784163\n",
      "train loss:0.355345791477\n",
      "train loss:0.426044659564\n",
      "train loss:0.544869906018\n",
      "train loss:0.348574747427\n",
      "train loss:0.479692762241\n",
      "train loss:0.35777852902\n",
      "train loss:0.487020263523\n",
      "train loss:0.370945837559\n",
      "train loss:0.604860599112\n",
      "train loss:0.390754873068\n",
      "train loss:0.452629676178\n",
      "train loss:0.343110870534\n",
      "train loss:0.410646723105\n",
      "train loss:0.278031062124\n",
      "train loss:0.576404486703\n",
      "train loss:0.467630459185\n",
      "train loss:0.582416213293\n",
      "train loss:0.421145875423\n",
      "train loss:0.445472711241\n",
      "train loss:0.339933774349\n",
      "train loss:0.487708688317\n",
      "train loss:0.426019356908\n",
      "train loss:0.515932429304\n",
      "train loss:0.256390485312\n",
      "train loss:0.517097559991\n",
      "train loss:0.496268747769\n",
      "train loss:0.532808070507\n",
      "train loss:0.397527742877\n",
      "train loss:0.393320444393\n",
      "train loss:0.262949502518\n",
      "train loss:0.423796997177\n",
      "train loss:0.402273949195\n",
      "train loss:0.441794315465\n",
      "train loss:0.437761548915\n",
      "train loss:0.27749166763\n",
      "train loss:0.412875609435\n",
      "train loss:0.316857011958\n",
      "train loss:0.426185761275\n",
      "train loss:0.580253245973\n",
      "train loss:0.304805918927\n",
      "train loss:0.38206269389\n",
      "train loss:0.361451130073\n",
      "train loss:0.490313652888\n",
      "train loss:0.381333895103\n",
      "train loss:0.423490336371\n",
      "train loss:0.418066994401\n",
      "train loss:0.399514005082\n",
      "train loss:0.326168759665\n",
      "train loss:0.427495767939\n",
      "train loss:0.339390649949\n",
      "train loss:0.37970911706\n",
      "train loss:0.284358295685\n",
      "train loss:0.428089268692\n",
      "train loss:0.43329366704\n",
      "train loss:0.325503492183\n",
      "train loss:0.535848507819\n",
      "train loss:0.396375047851\n",
      "train loss:0.297977487838\n",
      "train loss:0.337323430042\n",
      "train loss:0.389104262317\n",
      "train loss:0.498696726844\n",
      "train loss:0.342504803634\n",
      "train loss:0.277538407711\n",
      "train loss:0.240373804246\n",
      "train loss:0.250526585503\n",
      "train loss:0.339220170015\n",
      "train loss:0.197554297766\n",
      "train loss:0.30028945368\n",
      "train loss:0.338112777848\n",
      "train loss:0.320253260617\n",
      "train loss:0.372376274943\n",
      "train loss:0.409025870636\n",
      "train loss:0.475639614754\n",
      "train loss:0.602433178022\n",
      "train loss:0.323290553656\n",
      "train loss:0.373789530551\n",
      "train loss:0.361992114995\n",
      "train loss:0.528302671203\n",
      "train loss:0.330022183839\n",
      "train loss:0.351611415705\n",
      "train loss:0.399653224355\n",
      "train loss:0.298499882818\n",
      "train loss:0.506478431056\n",
      "train loss:0.252667766888\n",
      "train loss:0.31724024084\n",
      "train loss:0.264643045964\n",
      "train loss:0.326392388427\n",
      "train loss:0.28290156094\n",
      "train loss:0.224654846702\n",
      "train loss:0.52555693255\n",
      "train loss:0.334135078178\n",
      "train loss:0.162127641231\n",
      "train loss:0.122045034329\n",
      "train loss:0.222799912191\n",
      "train loss:0.291906353078\n",
      "train loss:0.347983896031\n",
      "train loss:0.194311728068\n",
      "train loss:0.293917158025\n",
      "train loss:0.238888684733\n",
      "train loss:0.23468865864\n",
      "train loss:0.323204127411\n",
      "train loss:0.230477619131\n",
      "train loss:0.260959593302\n",
      "train loss:0.337976725802\n",
      "train loss:0.229242004871\n",
      "train loss:0.207757767787\n",
      "train loss:0.261167227004\n",
      "train loss:0.456752913421\n",
      "train loss:0.223438985045\n",
      "train loss:0.211886177116\n",
      "train loss:0.27882661173\n",
      "train loss:0.243435213362\n",
      "train loss:0.277220030206\n",
      "train loss:0.263585745732\n",
      "train loss:0.286527834975\n",
      "train loss:0.326962274119\n",
      "train loss:0.237039298654\n",
      "train loss:0.28019732855\n",
      "train loss:0.223318414479\n",
      "train loss:0.28243591687\n",
      "train loss:0.170684223736\n",
      "train loss:0.371124059633\n",
      "train loss:0.279559281974\n",
      "train loss:0.312617007053\n",
      "train loss:0.240394561356\n",
      "train loss:0.455127246793\n",
      "train loss:0.238194978033\n",
      "train loss:0.290461344927\n",
      "train loss:0.194436300617\n",
      "train loss:0.287352573708\n",
      "train loss:0.235527912284\n",
      "train loss:0.35393152807\n",
      "train loss:0.403647206384\n",
      "train loss:0.182400785141\n",
      "train loss:0.274367271189\n",
      "train loss:0.276418639863\n",
      "train loss:0.287273107527\n",
      "train loss:0.156958574353\n",
      "train loss:0.314780053197\n",
      "train loss:0.219048101804\n",
      "train loss:0.214805984893\n",
      "train loss:0.320140543191\n",
      "train loss:0.278961804526\n",
      "train loss:0.320266225402\n",
      "train loss:0.374675445323\n",
      "train loss:0.253466451214\n",
      "train loss:0.296574581393\n",
      "train loss:0.232048901567\n",
      "train loss:0.288754441243\n",
      "train loss:0.270637235425\n",
      "train loss:0.290092164711\n",
      "train loss:0.274726464708\n",
      "train loss:0.38719323917\n",
      "train loss:0.160765764887\n",
      "train loss:0.151019475335\n",
      "train loss:0.318243958241\n",
      "train loss:0.175563203133\n",
      "train loss:0.160505912002\n",
      "train loss:0.339953569145\n",
      "train loss:0.25999828787\n",
      "train loss:0.378856412049\n",
      "train loss:0.20439948082\n",
      "train loss:0.237269273344\n",
      "train loss:0.358713704607\n",
      "train loss:0.252087923715\n",
      "train loss:0.301497795895\n",
      "train loss:0.174146835183\n",
      "train loss:0.36520955275\n",
      "train loss:0.185472098987\n",
      "train loss:0.311755131098\n",
      "train loss:0.308308643448\n",
      "train loss:0.466549749764\n",
      "train loss:0.205255307367\n",
      "train loss:0.179450945277\n",
      "train loss:0.185614704837\n",
      "train loss:0.346545318015\n",
      "train loss:0.304735015449\n",
      "train loss:0.222403673844\n",
      "train loss:0.275515939965\n",
      "train loss:0.197062921421\n",
      "train loss:0.231564165388\n",
      "train loss:0.170406365557\n",
      "train loss:0.164060651538\n",
      "train loss:0.1846659719\n",
      "train loss:0.324107709922\n",
      "train loss:0.255290303341\n",
      "train loss:0.292956537671\n",
      "train loss:0.231834593762\n",
      "train loss:0.224477824638\n",
      "train loss:0.306924447522\n",
      "train loss:0.188693703933\n",
      "train loss:0.13295588761\n",
      "train loss:0.169680724463\n",
      "train loss:0.149689564969\n",
      "train loss:0.211564959197\n",
      "train loss:0.209416414299\n",
      "train loss:0.19531754368\n",
      "train loss:0.245707755903\n",
      "train loss:0.262471057332\n",
      "train loss:0.349350700643\n",
      "train loss:0.175912553628\n",
      "train loss:0.180191230773\n",
      "train loss:0.160080799179\n",
      "train loss:0.229422495456\n",
      "train loss:0.270398381781\n",
      "train loss:0.225119073345\n",
      "train loss:0.106677949677\n",
      "train loss:0.0823325902063\n",
      "train loss:0.274757889691\n",
      "train loss:0.179537247521\n",
      "train loss:0.314672961353\n",
      "train loss:0.145910984075\n",
      "train loss:0.152693817812\n",
      "train loss:0.177995060945\n",
      "train loss:0.154179327149\n",
      "train loss:0.250405870699\n",
      "train loss:0.279852714085\n",
      "train loss:0.235258409542\n",
      "train loss:0.148409369902\n",
      "train loss:0.335918114732\n",
      "train loss:0.169449474682\n",
      "train loss:0.166552878888\n",
      "train loss:0.155824176829\n",
      "train loss:0.218842981295\n",
      "train loss:0.181201965705\n",
      "train loss:0.221929078988\n",
      "train loss:0.284555817896\n",
      "train loss:0.175215797336\n",
      "train loss:0.354443048635\n",
      "train loss:0.23941202664\n",
      "train loss:0.123045575881\n",
      "train loss:0.280662608191\n",
      "train loss:0.236021888124\n",
      "train loss:0.246959224878\n",
      "train loss:0.416121117146\n",
      "train loss:0.274876954327\n",
      "train loss:0.240565289435\n",
      "train loss:0.217763538859\n",
      "train loss:0.243458952295\n",
      "train loss:0.194318717577\n",
      "train loss:0.118449247069\n",
      "train loss:0.156208807622\n",
      "train loss:0.154734607096\n",
      "train loss:0.452340420665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.199182414608\n",
      "train loss:0.188224213419\n",
      "train loss:0.338752163804\n",
      "train loss:0.298441289706\n",
      "train loss:0.188846014907\n",
      "train loss:0.279361884812\n",
      "train loss:0.13936632684\n",
      "train loss:0.228002842682\n",
      "train loss:0.235690694146\n",
      "train loss:0.155697448005\n",
      "train loss:0.162027794356\n",
      "train loss:0.161821484478\n",
      "train loss:0.105036514301\n",
      "train loss:0.155183819589\n",
      "train loss:0.199474422564\n",
      "train loss:0.178727484518\n",
      "train loss:0.14637596796\n",
      "train loss:0.166152914151\n",
      "train loss:0.234538179039\n",
      "train loss:0.182165691787\n",
      "train loss:0.237921377026\n",
      "train loss:0.371741488282\n",
      "train loss:0.162254963943\n",
      "train loss:0.187891110906\n",
      "train loss:0.150909610879\n",
      "train loss:0.0972380993208\n",
      "train loss:0.156432303066\n",
      "train loss:0.26403281153\n",
      "train loss:0.182469454249\n",
      "train loss:0.0986637499152\n",
      "train loss:0.0833614808708\n",
      "train loss:0.181911105967\n",
      "train loss:0.188309385413\n",
      "train loss:0.339524059767\n",
      "train loss:0.13803136667\n",
      "train loss:0.229634613836\n",
      "train loss:0.0666237956553\n",
      "train loss:0.245825341675\n",
      "train loss:0.30414483679\n",
      "train loss:0.174655868285\n",
      "train loss:0.128709770287\n",
      "train loss:0.205024012435\n",
      "train loss:0.142769308759\n",
      "train loss:0.164726696847\n",
      "train loss:0.235677623356\n",
      "train loss:0.205839358283\n",
      "train loss:0.175594238214\n",
      "train loss:0.13208024315\n",
      "train loss:0.259862927371\n",
      "train loss:0.186375034102\n",
      "train loss:0.169003334869\n",
      "train loss:0.170190706492\n",
      "train loss:0.262080171728\n",
      "train loss:0.169722719756\n",
      "train loss:0.21110796623\n",
      "train loss:0.168724302596\n",
      "train loss:0.114917299746\n",
      "train loss:0.288991971163\n",
      "train loss:0.144587673565\n",
      "train loss:0.0870742154051\n",
      "train loss:0.18345053896\n",
      "train loss:0.105188727858\n",
      "train loss:0.165070744343\n",
      "train loss:0.152143732791\n",
      "train loss:0.20949861226\n",
      "train loss:0.144121376293\n",
      "train loss:0.115875953746\n",
      "train loss:0.268032820247\n",
      "train loss:0.17311270982\n",
      "train loss:0.188312123072\n",
      "train loss:0.0828101549688\n",
      "train loss:0.114285882616\n",
      "train loss:0.167727036791\n",
      "train loss:0.0778282625655\n",
      "train loss:0.147267508726\n",
      "train loss:0.182301868009\n",
      "train loss:0.140822022451\n",
      "train loss:0.179218466035\n",
      "train loss:0.130030086307\n",
      "train loss:0.133660847022\n",
      "train loss:0.304816714415\n",
      "train loss:0.142361996923\n",
      "train loss:0.229619051524\n",
      "train loss:0.109443785908\n",
      "train loss:0.125464579474\n",
      "train loss:0.117088113612\n",
      "train loss:0.0881861471404\n",
      "train loss:0.183827949559\n",
      "train loss:0.182924814013\n",
      "train loss:0.218961469006\n",
      "train loss:0.136525826909\n",
      "train loss:0.27383503881\n",
      "train loss:0.151936311902\n",
      "train loss:0.183137997728\n",
      "train loss:0.185592186207\n",
      "train loss:0.297392168903\n",
      "train loss:0.236062845378\n",
      "train loss:0.275680507326\n",
      "train loss:0.167438921812\n",
      "train loss:0.130519858371\n",
      "train loss:0.171225620175\n",
      "train loss:0.146720764397\n",
      "train loss:0.161006768073\n",
      "train loss:0.199794083861\n",
      "train loss:0.136490052146\n",
      "train loss:0.174735417675\n",
      "train loss:0.134639330637\n",
      "train loss:0.264516647283\n",
      "train loss:0.131338658714\n",
      "train loss:0.224485356631\n",
      "train loss:0.222576626121\n",
      "train loss:0.0842902786917\n",
      "train loss:0.196631365346\n",
      "train loss:0.113903073566\n",
      "train loss:0.235219947904\n",
      "train loss:0.0968947208135\n",
      "train loss:0.126495697708\n",
      "train loss:0.153879311824\n",
      "train loss:0.174567488342\n",
      "train loss:0.154582248116\n",
      "train loss:0.142768888367\n",
      "train loss:0.127064246135\n",
      "train loss:0.151550845918\n",
      "train loss:0.23012699259\n",
      "train loss:0.121250482748\n",
      "train loss:0.140061979788\n",
      "train loss:0.147620669535\n",
      "train loss:0.260384394729\n",
      "train loss:0.121031081995\n",
      "train loss:0.214119017098\n",
      "train loss:0.178465973848\n",
      "train loss:0.151356882839\n",
      "train loss:0.104121269336\n",
      "train loss:0.18731767998\n",
      "train loss:0.103358770611\n",
      "train loss:0.122541511531\n",
      "train loss:0.200282316626\n",
      "train loss:0.156988608701\n",
      "train loss:0.160435009289\n",
      "train loss:0.0999429185259\n",
      "train loss:0.124782335902\n",
      "train loss:0.155900780677\n",
      "train loss:0.176848139039\n",
      "train loss:0.103672566742\n",
      "train loss:0.156436474749\n",
      "train loss:0.141401475835\n",
      "train loss:0.0980502797818\n",
      "train loss:0.081577321454\n",
      "train loss:0.180413841926\n",
      "train loss:0.0892734812142\n",
      "train loss:0.0984346721726\n",
      "train loss:0.112638741542\n",
      "train loss:0.0733369437748\n",
      "train loss:0.0726008413347\n",
      "train loss:0.127794619514\n",
      "train loss:0.0786589729742\n",
      "train loss:0.147883046494\n",
      "train loss:0.124386491774\n",
      "train loss:0.0925330494497\n",
      "train loss:0.106558995334\n",
      "train loss:0.0667307604725\n",
      "train loss:0.191951044619\n",
      "train loss:0.12689667878\n",
      "train loss:0.0296507173953\n",
      "train loss:0.108620047709\n",
      "train loss:0.0914719053201\n",
      "train loss:0.173672267116\n",
      "train loss:0.0850291173832\n",
      "train loss:0.200334145371\n",
      "train loss:0.096533892457\n",
      "train loss:0.208101314951\n",
      "train loss:0.126630769314\n",
      "train loss:0.0916208604329\n",
      "train loss:0.121147514882\n",
      "train loss:0.117613898609\n",
      "train loss:0.218294181923\n",
      "train loss:0.227698661026\n",
      "train loss:0.183633685871\n",
      "train loss:0.115055369548\n",
      "train loss:0.120949365879\n",
      "train loss:0.131413596106\n",
      "train loss:0.114119328253\n",
      "train loss:0.154102539736\n",
      "train loss:0.120355795808\n",
      "train loss:0.0686791191558\n",
      "train loss:0.0850345362849\n",
      "train loss:0.0736038470765\n",
      "train loss:0.179098938537\n",
      "train loss:0.0657014685038\n",
      "train loss:0.0883761307305\n",
      "train loss:0.0969445150732\n",
      "train loss:0.0670575106093\n",
      "train loss:0.0912954477877\n",
      "train loss:0.107014946639\n",
      "train loss:0.262757545366\n",
      "train loss:0.162010446533\n",
      "train loss:0.0870935827664\n",
      "train loss:0.0802635347807\n",
      "train loss:0.15235347114\n",
      "train loss:0.135169385601\n",
      "train loss:0.0798828736306\n",
      "train loss:0.092855089253\n",
      "train loss:0.154896054408\n",
      "train loss:0.22893587608\n",
      "train loss:0.152645727112\n",
      "train loss:0.0902718769523\n",
      "train loss:0.175142632692\n",
      "train loss:0.118692502636\n",
      "train loss:0.0860213252025\n",
      "train loss:0.107181240031\n",
      "train loss:0.157709379925\n",
      "train loss:0.138276308943\n",
      "train loss:0.12927583172\n",
      "train loss:0.160964820711\n",
      "train loss:0.12411912113\n",
      "train loss:0.18237949978\n",
      "train loss:0.0793758431231\n",
      "train loss:0.227174169222\n",
      "train loss:0.0790157672664\n",
      "train loss:0.16539041152\n",
      "train loss:0.172725440249\n",
      "train loss:0.0948629894765\n",
      "train loss:0.100224416962\n",
      "train loss:0.363941102904\n",
      "train loss:0.133668112594\n",
      "train loss:0.124565046667\n",
      "train loss:0.195420983523\n",
      "train loss:0.161086502396\n",
      "train loss:0.155667912738\n",
      "train loss:0.271832468131\n",
      "train loss:0.0692101769433\n",
      "train loss:0.171759311674\n",
      "train loss:0.140803356206\n",
      "train loss:0.220649527482\n",
      "train loss:0.12693641141\n",
      "train loss:0.261666899554\n",
      "train loss:0.189954672554\n",
      "train loss:0.106124304908\n",
      "train loss:0.0493019043463\n",
      "train loss:0.0503232639928\n",
      "train loss:0.0542340451151\n",
      "train loss:0.205436762378\n",
      "train loss:0.0725410641389\n",
      "train loss:0.0509200690999\n",
      "train loss:0.0808313654208\n",
      "train loss:0.116710389218\n",
      "train loss:0.112615212408\n",
      "train loss:0.187769179212\n",
      "train loss:0.0620399802413\n",
      "train loss:0.189033612054\n",
      "train loss:0.120394858293\n",
      "train loss:0.160604074954\n",
      "train loss:0.0812915277456\n",
      "train loss:0.0932801708972\n",
      "train loss:0.157624195726\n",
      "train loss:0.214438032973\n",
      "train loss:0.118077086564\n",
      "train loss:0.0687793047087\n",
      "train loss:0.112637324517\n",
      "train loss:0.142451588858\n",
      "train loss:0.0992682174094\n",
      "train loss:0.137262698329\n",
      "train loss:0.120376157171\n",
      "train loss:0.0644022234193\n",
      "train loss:0.0446289334375\n",
      "train loss:0.091813021826\n",
      "train loss:0.142732949549\n",
      "train loss:0.141645447466\n",
      "train loss:0.206793983782\n",
      "train loss:0.17715629912\n",
      "train loss:0.103163647806\n",
      "train loss:0.145456539097\n",
      "train loss:0.14141357704\n",
      "train loss:0.0648992022526\n",
      "train loss:0.0948881515916\n",
      "train loss:0.106990519599\n",
      "train loss:0.0932851353143\n",
      "train loss:0.123373641495\n",
      "train loss:0.134863734115\n",
      "train loss:0.217533542094\n",
      "train loss:0.122749090635\n",
      "train loss:0.106041637343\n",
      "train loss:0.219749263592\n",
      "train loss:0.172988328584\n",
      "train loss:0.146406500348\n",
      "=== epoch:2, train acc:0.96, test acc:0.967 ===\n",
      "train loss:0.0970316113463\n",
      "train loss:0.0936352310442\n",
      "train loss:0.131317991263\n",
      "train loss:0.107819696021\n",
      "train loss:0.105925822755\n",
      "train loss:0.116767217383\n",
      "train loss:0.0920911556932\n",
      "train loss:0.069162142257\n",
      "train loss:0.125079967993\n",
      "train loss:0.0927194327262\n",
      "train loss:0.0831052764138\n",
      "train loss:0.115731522357\n",
      "train loss:0.0849117554297\n",
      "train loss:0.347409195616\n",
      "train loss:0.112483744322\n",
      "train loss:0.066746027212\n",
      "train loss:0.0878428624233\n",
      "train loss:0.174425622131\n",
      "train loss:0.108221454968\n",
      "train loss:0.173528286223\n",
      "train loss:0.161302416986\n",
      "train loss:0.22935723696\n",
      "train loss:0.0833678375624\n",
      "train loss:0.109687160052\n",
      "train loss:0.062293998489\n",
      "train loss:0.259343304995\n",
      "train loss:0.0590552710051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.16203849658\n",
      "train loss:0.0847128437727\n",
      "train loss:0.117833252307\n",
      "train loss:0.0991971866639\n",
      "train loss:0.115128272818\n",
      "train loss:0.128209017005\n",
      "train loss:0.187799624281\n",
      "train loss:0.088585948457\n",
      "train loss:0.0847154380915\n",
      "train loss:0.259228722923\n",
      "train loss:0.144138169986\n",
      "train loss:0.119046145825\n",
      "train loss:0.164655968628\n",
      "train loss:0.159545728935\n",
      "train loss:0.0995289040491\n",
      "train loss:0.0635226399413\n",
      "train loss:0.10888688461\n",
      "train loss:0.144175772193\n",
      "train loss:0.10553298336\n",
      "train loss:0.057168261444\n",
      "train loss:0.0530401446768\n",
      "train loss:0.0934062411107\n",
      "train loss:0.143436881081\n",
      "train loss:0.119586708837\n",
      "train loss:0.119511469643\n",
      "train loss:0.111915818853\n",
      "train loss:0.064349383576\n",
      "train loss:0.0846751641976\n",
      "train loss:0.230637539319\n",
      "train loss:0.0497390451339\n",
      "train loss:0.132786808706\n",
      "train loss:0.115492769404\n",
      "train loss:0.253633203378\n",
      "train loss:0.0425220683896\n",
      "train loss:0.0727113738344\n",
      "train loss:0.119910502044\n",
      "train loss:0.281315332021\n",
      "train loss:0.0649975499608\n",
      "train loss:0.110503068618\n",
      "train loss:0.0928160451991\n",
      "train loss:0.148223036175\n",
      "train loss:0.255291167205\n",
      "train loss:0.119974843569\n",
      "train loss:0.135393400018\n",
      "train loss:0.0803846585527\n",
      "train loss:0.141539480621\n",
      "train loss:0.0477124292008\n",
      "train loss:0.0885607190815\n",
      "train loss:0.172442395698\n",
      "train loss:0.197390083526\n",
      "train loss:0.0783900286344\n",
      "train loss:0.116851882375\n",
      "train loss:0.0874647069223\n",
      "train loss:0.154954984238\n",
      "train loss:0.124742888987\n",
      "train loss:0.0754295641232\n",
      "train loss:0.0850093115492\n",
      "train loss:0.138629751195\n",
      "train loss:0.125216861024\n",
      "train loss:0.18417140322\n",
      "train loss:0.100019470875\n",
      "train loss:0.0864022360277\n",
      "train loss:0.189553826946\n",
      "train loss:0.117698743605\n",
      "train loss:0.138039249148\n",
      "train loss:0.0967596950905\n",
      "train loss:0.137076061919\n",
      "train loss:0.110126506378\n",
      "train loss:0.0305872236799\n",
      "train loss:0.143226402518\n",
      "train loss:0.100879252843\n",
      "train loss:0.12428147522\n",
      "train loss:0.0693748834686\n",
      "train loss:0.127031627096\n",
      "train loss:0.147561540644\n",
      "train loss:0.135229624796\n",
      "train loss:0.14575029496\n",
      "train loss:0.109275025437\n",
      "train loss:0.0911805462232\n",
      "train loss:0.0449163110851\n",
      "train loss:0.154189146092\n",
      "train loss:0.0412850237607\n",
      "train loss:0.0917476607533\n",
      "train loss:0.044083325115\n",
      "train loss:0.146360732657\n",
      "train loss:0.0629555335584\n",
      "train loss:0.0647738805627\n",
      "train loss:0.0931891416938\n",
      "train loss:0.181945537757\n",
      "train loss:0.170576591598\n",
      "train loss:0.128333428483\n",
      "train loss:0.144752353272\n",
      "train loss:0.200123860926\n",
      "train loss:0.04912706441\n",
      "train loss:0.0839215599458\n",
      "train loss:0.166426690434\n",
      "train loss:0.106135772466\n",
      "train loss:0.0965871130438\n",
      "train loss:0.0774350415611\n",
      "train loss:0.116840572378\n",
      "train loss:0.0665689550131\n",
      "train loss:0.135501621285\n",
      "train loss:0.106168746652\n",
      "train loss:0.0832667141753\n",
      "train loss:0.0635909176925\n",
      "train loss:0.0276070719883\n",
      "train loss:0.172202627416\n",
      "train loss:0.117932304429\n",
      "train loss:0.133587164839\n",
      "train loss:0.0873237430172\n",
      "train loss:0.0543243853627\n",
      "train loss:0.0576083350907\n",
      "train loss:0.0689582103683\n",
      "train loss:0.112922372661\n",
      "train loss:0.0321620738333\n",
      "train loss:0.0600468962451\n",
      "train loss:0.150510282316\n",
      "train loss:0.106613990704\n",
      "train loss:0.0978774096357\n",
      "train loss:0.153265552944\n",
      "train loss:0.0370275299795\n",
      "train loss:0.0985174606488\n",
      "train loss:0.0866191469288\n",
      "train loss:0.115953135812\n",
      "train loss:0.0412201201281\n",
      "train loss:0.0600863143921\n",
      "train loss:0.0485379498429\n",
      "train loss:0.142638483314\n",
      "train loss:0.0848173760429\n",
      "train loss:0.160018052697\n",
      "train loss:0.0827860928536\n",
      "train loss:0.1122491407\n",
      "train loss:0.0540846340728\n",
      "train loss:0.0753406719589\n",
      "train loss:0.0114010606522\n",
      "train loss:0.165415943377\n",
      "train loss:0.143188424474\n",
      "train loss:0.0686324659328\n",
      "train loss:0.0845203108866\n",
      "train loss:0.0728239481629\n",
      "train loss:0.153442649826\n",
      "train loss:0.0744098062994\n",
      "train loss:0.131940184255\n",
      "train loss:0.0604705289456\n",
      "train loss:0.0855912032574\n",
      "train loss:0.0499157210592\n",
      "train loss:0.0603805325669\n",
      "train loss:0.0460936152989\n",
      "train loss:0.0806462201071\n",
      "train loss:0.0221594060239\n",
      "train loss:0.152804260843\n",
      "train loss:0.0909023537433\n",
      "train loss:0.0753183209595\n",
      "train loss:0.0501947376798\n",
      "train loss:0.0878257155895\n",
      "train loss:0.08038886027\n",
      "train loss:0.183851820714\n",
      "train loss:0.105653679129\n",
      "train loss:0.090709857442\n",
      "train loss:0.111295655793\n",
      "train loss:0.0829020231887\n",
      "train loss:0.135168287291\n",
      "train loss:0.124991411411\n",
      "train loss:0.0517940207163\n",
      "train loss:0.0842593257848\n",
      "train loss:0.0779145914616\n",
      "train loss:0.114575059871\n",
      "train loss:0.0652590436009\n",
      "train loss:0.0653965711653\n",
      "train loss:0.0875473314836\n",
      "train loss:0.0545536205162\n",
      "train loss:0.044038825534\n",
      "train loss:0.0528625105669\n",
      "train loss:0.0536761825403\n",
      "train loss:0.0765197021657\n",
      "train loss:0.234127375123\n",
      "train loss:0.075840250506\n",
      "train loss:0.0915205431315\n",
      "train loss:0.0633920158047\n",
      "train loss:0.0263139170275\n",
      "train loss:0.0454555925978\n",
      "train loss:0.0576524089851\n",
      "train loss:0.0974379841356\n",
      "train loss:0.110159244989\n",
      "train loss:0.101912609932\n",
      "train loss:0.0651902522245\n",
      "train loss:0.122474734002\n",
      "train loss:0.0836931551679\n",
      "train loss:0.0558769470093\n",
      "train loss:0.0613178682836\n",
      "train loss:0.158710460171\n",
      "train loss:0.069190390081\n",
      "train loss:0.113854197768\n",
      "train loss:0.0423671223731\n",
      "train loss:0.0359451415956\n",
      "train loss:0.0759512291822\n",
      "train loss:0.0476159325455\n",
      "train loss:0.0777342981029\n",
      "train loss:0.0677972997991\n",
      "train loss:0.0751276800404\n",
      "train loss:0.065221618494\n",
      "train loss:0.0895259437413\n",
      "train loss:0.115927179942\n",
      "train loss:0.120590380488\n",
      "train loss:0.0580407506075\n",
      "train loss:0.119746276969\n",
      "train loss:0.0762050950663\n",
      "train loss:0.0780628909567\n",
      "train loss:0.078336457792\n",
      "train loss:0.080617868204\n",
      "train loss:0.0635194706083\n",
      "train loss:0.076861497954\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3bfa0a9c5d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# パラメータの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saito/work/reading_circle_deep_learning_sample/deep_learning_from_scratch/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saito/work/reading_circle_deep_learning_sample/deep_learning_from_scratch/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8b7eb2c4a2dc>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \"\"\"\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8b7eb2c4a2dc>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0m引数のxは入力データ\u001b[0m\u001b[0;31m、\u001b[0m\u001b[0mtは教師ラベル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8b7eb2c4a2dc>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-77da568f5a28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-49a3237c8648>\u001b[0m in \u001b[0;36mim2col\u001b[0;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 入力画像の縦横(インデックス2と3)の両端にpad個ずつ定数(0)をパディング\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 展開後の入力データを格納する行列 col の生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# (参考) https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/saito/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[0;31m# If we get here, use new padding method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m     \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0;31m# API preserved, but completely new algorithm which pads by building the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 CNNの可視化\n",
    "* 畳み込み層を可視化することでCNNで何が行われているのかを探索する\n",
    "### 7.6.1 1層目の重みの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad8a550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC/Tc+lHNBWrSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad7f978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2 階層構造による情報抽出\n",
    "* 先の結果の考察\n",
    "  * 1層目の畳み込み層を対象とした結果である\n",
    "  * 1層目ではエッジやブロブなどの低レベルな情報が抽出される\n",
    "  * ディープラーニングの可視化に関する研究では，層が深くなるに従い抽出される情報はより抽象化されていくことが示されている\n",
    "    * AlexNetのネットワーク構成は畳み込み層とプーリング層が何層にも重なり，最後に全結合層を経て結果が出力される\n",
    "  * 畳み込み層を何層も重ねると，層が深くなるにつれ，より複雑で抽象化された情報が抽出される\n",
    "  * 最初の層は単純なエッジに反応し，次にテクスチャに反応し，そしてより複雑な物体のパーツへと反応するように変化する\n",
    "    * 層が深くなるに従い，単純な形状から高度な情報へと変化する．\n",
    "    * ものの意味を理解するかのように反応する対象が変化する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 代表的なCNN\n",
    "* CNNは様々な構成のネットワークが提案されてきた\n",
    "* LeNetおよびAlexNetは特に重要なネットワークである\n",
    "\n",
    "### 7.7.1 LeNet\n",
    "* CNNの元祖\n",
    "* 手書き数字認識を行うネットワークとして1998年に提案された\n",
    "* 畳み込み層とサブサンプリング層（要素を間引く層）を連続して行い，最後に全結合層を経て結果が出力される\n",
    "* 現在のCNNとの違い\n",
    "  * シグモイド関数が使われている（現在は主にReLUが使われる）\n",
    "  * サブサンプリングによって中間データのサイズを縮小している（現在はMaxプーリングが主流） \n",
    "\n",
    "### 7.7.2 AlexNet\n",
    "* ディープラーニングが注目されるに至ったCNN．2012年に提案された．\n",
    "* 畳み込み層とプーリング層を重ね，最後に全結合層を経由して結果を出力する\n",
    "* 基本構成はLeNetと変わらないが，以下の点で異なる\n",
    "  * ReLUを使用する\n",
    "  * LRN(Local Response Normalization)という局所的正規化の層を使用する\n",
    "  * Dropoutを使用する\n",
    "* LeNetの時代と比べるとコンピューティング環境が変化\n",
    "  * 大量データの入手が容易になった\n",
    "  * GPUが普及し，大量の演算を高速に実行可能となった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 まとめ\n",
    "* CNNについて学んだ\n",
    "  * CNNを構成する基本モジュールは「畳み込み層」と「プーリング層」である\n",
    "  * CNNはこれまでの全結合層のネットワークに加え，畳み込み層とプーリング層が加わったもの\n",
    "  * CNNは画像を扱う分野では例外なく使われる\n",
    "* 畳み込み層とプーリング層はim2colを用いるとシンプルで効率よく実装できる\n",
    "* CNNの可視化により高度な情報が層を深くするにつれて抽出される様子がわかる\n",
    "* ディープラーニングの発展にビックデータとGPUが大きく貢献している"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
